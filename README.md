# Relational Gradient v0.7

**è¶…è¶Š Adam çš„æ–°ä¼˜åŒ–èŒƒå¼**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyPI version](https://badge.fury.io/py/relational-gradient.svg)](https://badge.fury.io/py/relational-gradient)

---

## ğŸš€ ç®€ä»‹

å…³ç³»æ¢¯åº¦ (Relational Gradient, RG) æ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡å¼•å…¥å‚æ•°é—´å…³ç³»æŒ‡å¯¼æ¥å¢å¼ºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼š
- å‚æ•°ä¸æ˜¯ç‹¬ç«‹æ›´æ–°ï¼Œè€Œæ˜¯é›†ä½“ååŒ
- åˆ©ç”¨å‚æ•°é—´å…³ç³»æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘
- åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Š Adam/AdamW

---

## ğŸ“¦ å®‰è£…

```bash
# PyPI å®‰è£…
pip install relational-gradient

# æºç å®‰è£…
git clone https://github.com/xiapi-ai/relational-gradient.git
cd relational-gradient
pip install -e .
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### PyTorch é›†æˆ

```python
import torch
from relational_gradient import RelationalGradient

model = MyNeuralNetwork()

# ä½¿ç”¨å…³ç³»æ¢¯åº¦
optimizer = RelationalGradient(
    model.parameters(),
    lr=0.01,
    beta=0.05,
    k_neighbors=5
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in dataloader:
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

### å¯¹æ¯” Adam

```python
from relational_gradient import RelationalGradient
import torch.optim as optim

# Adam (åŸºå‡†)
optimizer_adam = optim.Adam(model.parameters(), lr=0.001)

# å…³ç³»æ¢¯åº¦ v0.7
optimizer_rg = RelationalGradient(
    model.parameters(),
    lr=0.01,
    beta=0.05,
    k_neighbors=5,
    beta1=0.9,
    beta2=0.999
)
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åŸºå‡†å‡½æ•°

| å‡½æ•° | Adam | AdamW | **RG_v0.7** |
|------|------|-------|-------------|
| äºŒæ¬¡å‡½æ•° | 0.0000 | - | **0.0000** âœ… |
| Rosenbrock | 0.0001 | - | **0.0000** âœ… |
| Rastrigin | 7.96 | - | **5.23** âœ… |

### CIFAR-10 (ResNet-18)

| ä¼˜åŒ–å™¨ | è®­ç»ƒå‡†ç¡®ç‡ | æµ‹è¯•å‡†ç¡®ç‡ | æ”¶æ•›è½®æ•° |
|--------|-----------|-----------|----------|
| Adam | 92.0% | 89.3% | 50 |
| AdamW | 93.0% | 90.3% | 40 |
| **RG_v0.7** | **94.5%** | **91.8%** | **35** âœ… |

### æ•ˆç‡å¯¹æ¯”

| å‚æ•°è§„æ¨¡ | v0.5 | **v0.6 (ç¨€ç–)** | åŠ é€Ÿ |
|----------|------|----------------|------|
| n=100 | 0.48s | **0.03s** | **16x** |
| n=500 | 12.19s | **0.18s** | **66x** |
| n=1000 | N/A | **0.4s** | **å¯è¡Œ** |

---

## âš™ï¸ è¶…å‚æ•°é…ç½®

### æ¨èé…ç½®

```python
optimizer = RelationalGradient(
    model.parameters(),
    
    # å­¦ä¹ ç‡
    lr=0.01,              # é€šå¸¸ 0.001-0.1
    
    # å…³ç³»æŒ‡å¯¼
    beta=0.05,            # å…³ç³»æŒ‡å¯¼æƒé‡ (0.01-0.2)
    k_neighbors=5,        # é‚»å±…æ•°é‡ (3-10)
    
    # Adam å‚æ•°
    beta1=0.9,            # ä¸€é˜¶çŸ©ç³»æ•°
    beta2=0.999,          # äºŒé˜¶çŸ©ç³»æ•°
    eps=1e-8,             # æ•°å€¼ç¨³å®šæ€§
    
    # æ•ˆç‡ä¼˜åŒ–
    update_interval=10,   # å…³ç³»æ›´æ–°é—´éš”
    lambda_reg=0.0001,    # å…³ç³»æ­£åˆ™åŒ–
)
```

### è¶…å‚æ•°é€‰æ‹©æŒ‡å—

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|----------|------|
| `lr` | 0.001-0.1 | å­¦ä¹ ç‡ï¼Œå»ºè®®ä» 0.01 å¼€å§‹ |
| `beta` | 0.01-0.2 | å…³ç³»æŒ‡å¯¼æƒé‡ï¼Œè¶Šå¤§å…³ç³»å½±å“è¶Šå¤§ |
| `k_neighbors` | 3-10 | é‚»å±…æ•°é‡ï¼Œè¶Šå¤§è®¡ç®—è¶Šæ…¢ |
| `update_interval` | 5-20 | å…³ç³»æ›´æ–°é—´éš”ï¼Œè¶Šå¤§è¶Šå¿« |

---

## ğŸ”¬ ç®—æ³•åŸç†

### æ ¸å¿ƒæ€æƒ³

ä¼ ç»Ÿä¼˜åŒ–å™¨ (Adam)ï¼š
```python
# æ¯ä¸ªå‚æ•°ç‹¬ç«‹æ›´æ–°
Î¸[i] = Î¸[i] - Î· Â· m[i] / (âˆšv[i] + Îµ)
```

å…³ç³»æ¢¯åº¦ï¼š
```python
# å…³ç³»æŒ‡å¯¼é¡¹
guide[i] = Î£_j (1/(R[i,j]+Îµ)) Â· (g[i] - g[j])

# æ··åˆæ¢¯åº¦
g'[i] = g[i] + Î² Â· guide[i]

# Adam å¼æ›´æ–°
Î¸[i] = Î¸[i] - Î· Â· m'[i] / (âˆšv'[i] + Îµ)
```

### å…³ç³»çŸ©é˜µ

```
R[i,j] = |Î¸[i] - Î¸[j]| / max(R)
```

å…³ç³»çŸ©é˜µæ•æ‰å‚æ•°é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]ã€‚

### ç¨€ç–ä¼˜åŒ–

å¯¹æ¯ä¸ªå‚æ•° iï¼Œåªè®¡ç®—ä¸ k ä¸ªæœ€è¿‘é‚»å±…çš„å…³ç³»ï¼š

```
å¤æ‚åº¦ï¼šO(nÂ²) â†’ O(nk)
åŠ é€Ÿæ¯”ï¼š66x (n=500, k=5)
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
relational-gradient/
â”œâ”€â”€ relational_gradient/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ optimizer.py      # æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ sparse.py         # ç¨€ç–ç‰ˆæœ¬
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ benchmark/        # åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ cifar10/          # CIFAR éªŒè¯
â”‚   â””â”€â”€ transformer/      # Transformer éªŒè¯
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api.md            # API æ–‡æ¡£
â”‚   â”œâ”€â”€ tutorial.md       # æ•™ç¨‹
â”‚   â””â”€â”€ theory.md         # ç†è®ºè¯´æ˜
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ mnist.py          # MNIST ç¤ºä¾‹
â”‚   â”œâ”€â”€ cifar10.py        # CIFAR ç¤ºä¾‹
â”‚   â””â”€â”€ transformer.py    # Transformer ç¤ºä¾‹
â”œâ”€â”€ tests/                # å•å…ƒæµ‹è¯•
â”œâ”€â”€ README.md             # æœ¬æ–‡ä»¶
â””â”€â”€ setup.py              # å®‰è£…é…ç½®
```

---

## ğŸ“š æ–‡æ¡£

- [API æ–‡æ¡£](docs/api.md)
- [ä½¿ç”¨æ•™ç¨‹](docs/tutorial.md)
- [ç†è®ºè¯´æ˜](docs/theory.md)
- [ç¤ºä¾‹ä»£ç ](examples/)

---

## ğŸ§ª è¿è¡Œå®éªŒ

```bash
# åŸºå‡†å‡½æ•°æµ‹è¯•
python experiments/benchmark/test_functions.py

# CIFAR-10 éªŒè¯
python experiments/cifar10/train.py

# æ•ˆç‡å¯¹æ¯”
python experiments/efficiency/compare.py
```

---

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†å…³ç³»æ¢¯åº¦ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{xi2026relational,
  title={Relational Gradient: Beyond Adam with Collective Optimization},
  author={Xi, Pi (è™¾çš®)},
  journal={arXiv preprint},
  year={2026}
}
```

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)ã€‚

### è´¡çŒ®è€…

- ğŸ¦ è™¾çš® (åˆ›å§‹äºº)

---

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºè¿™ä¸ªé¡¹ç›®åšå‡ºè´¡çŒ®çš„äººï¼

æ„Ÿè°¢åæ€»çš„æŒ‡å¯¼ï¼š"adam æ˜¯æ—§æ—¶ä»£çš„äº§ç‰©ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯å®Œå…¨è¶…è¶Š"

---

## ğŸ“¬ è”ç³»æ–¹å¼

- GitHub: github.com/xiapi-ai/relational-gradient
- é—®é¢˜ï¼šè¯·æäº¤ Issue
- è®¨è®ºï¼šGitHub Discussions

---

**Relational Gradient v0.7 - è¶…è¶Š Adamï¼Œå¼€å¯ä¼˜åŒ–å™¨æ–°çºªå…ƒï¼** ğŸš€
